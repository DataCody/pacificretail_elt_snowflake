## Pacific Retail
# Overview

PacificRetail is a rapidly growing e-commerce company operating in 15 countries across North America and Europe. With over 5 million active customers and a catalog of more than 100,000 products. 
# Challengs
PacificRetail is facing significant challenges in managing and analyzing its growing data volumes. One of the main challenges, data silos, customer data, product information, and transaction records are stored in separate system, making it difficult to gain a holistic view of the business. Another challenge is a data processing delays. Current batch processing method results in 24 hours delays for sales reports and other kind of analysis, which causes a hindrance in real-time decision making. Scalability is another problem. Their existing on-premises data warehouses struggles to handle the increasing data volumes, especially during peak sales period. Data quality issues like inconsistency in data formats and lack of a standardization across different countries causes problems in reporting. And lastly, analytical limitations. The current setup does not support advanced analytics or machine learning initiatives crucial for personalized marketing and demand forecasting. Hence, 
# The Goal
PacificRetail aims to implement a modern data engineering solution using Snowflake to address these challenges. It wanted to create a centralized, a single source of a truth by consolidating data from various sources into this Snowflake. It also looking a solution which can help to reduce data processing time to enable near to real-time reporting and analytics. Leveraging a Snowflake's cloud native architecture can help to grow and scale to any heights and can manage the growing data problems of PacificRetail. As a part of the solution, they also wanted to implement data quality checks and standardization processes within the data pipeline. And this Snowflake build solution can become a foundational for advanced analytics and machine learning projects. 
# Data Source
The customer data is in the form of a CSV file, which is exported daily from the CRM system. Another data source is a product catalog. The product catalog is in the form of JSON files, which is updated hourly from the inventory management system. And lastly, we have the transactional logs, which is in the form of the Parquet files, and these are generated in real time from the e-commerce platform. 

# Outcome
The expected outcome in a large enterprise solution is to build an Snowflake-based solution that can help in reduction in data processing time from 24 hours to 1 hour. Also, we are looking to build 99% accuracy in cross channel sales reporting. Expectation is it should be smart enough and powerful enough to handle the five times of current data volume without having performance degradation. It should also enable self-service analytics for business users across different platforms. And lastly, it should lay the foundation for ML models. 

# Implement
So the next question come in the mind is how are we going to implement? What could be the approach? So the approach would be having the multi-layer Snowflake architecture where we have three layers: the Bronze layer for data ingestion, Silver layer for clean and transform data, and Gold layer for business level aggregates and data mods. So this is the challenges and the expectation of the PacificRetail from this Snowflake-based solution. In the next chapter, we will dive into high level solution architecture and trying to find out how we can implement a Snowflake and what are the different features of the Snowflake which can help us to do this. See you into the next video.

# solution architecture
Our solution architecture consists of following key points, data sources, Azure Data Lake Storage, this is the place where the data is going to put by external system like CRM, inventory management and our order management. Then we have the Snowflake Data Warehouse, data processing layers and reporting and analytical tool. Let's understand these data sources. The PacificRetail data comes from various operational system. The CRM stands for Customer Relationship Management system where majorly the customer data is available then we have an inventory management system. From there, we are going to get the product catalog information. And lastly, we have an e-commerce platform where all the transaction happens and we get the order and transaction data from these systems. Next, is Azure Data Lake Storage. ADLS serves as our centralized data lake where all the raw data is initially stored. The importance of this ADLS is it provides the scalable and cost effective storage for large volumes of data. It can support variety of data formats, whether it is CSV, JSON, Parquet, it can store any kind of files and this can also act as the staging area before the data is going to get loaded into the Snowflake. Snowflake is the core of our data engineering solution. We are going to connect ADLS to the Snowflake via external stage. Then we are going to use the COPY command to ingest the data from ADLS to Snowflake tables. After that, we are going to process and transform the data as per the business logic. Then we are going to have a separate layer within the Snowflake in the form of Bronze, Silver, and Gold to store the data at the various levels. And this will provide the computation resources for data processing and analytics. 

# Data Processing Layers
In the data processing layers, we have the three layer architecture. Bronze is the layer where we going to put the raw data. This is the layer where we are not doing any transformation, any merging, any cleanup. It's like an upend mode only. All the rules is going to get added up into this layer. The idea is it is going to preserve the original structure and content of the data. Then comes the Silver layer. Silver layer is going to contain the clean and transformed data. It implements incremental merge logic to handle the updates. So if new rows are coming in, we are going to insert in the Silver layer. But if existing rows are coming in, we are going to update using the merge commands. The major significance of the Silver layer is this is the place where we are going to apply lot of data quality checks and standardization. Then comes the Gold layer. Gold layer is the place where we are going to keep the business level aggregates and the data match. This layer is optimized for a specific analytical use cases that provides a denormalized query-friendly structure. 
# Data Flow
If we talk about the data flow of this project, the data is going to get into the ADLS from the source system. Our work start after the data get landed up into the ADLS Snowflake task, uses the COPY command to load the data from external stages into the Bronze layer tables, and that is where the data get first time into the Snowflake. Once the data is clear in the Bronze layer within the Snowflake, we are going to run an another task, which has a (indistinct) rephrase, which has a purpose to apply data quality and all the transformation logic and push the data into the Silver layer. Finally, the set of tasks move the data from Silver layer to the Gold layer, creating the business specific aggregates and view. Lastly, if you have any Business Intelligence or a reporting tool, they can connect to the Snowflake and get the data from this Gold layer, to showcase in the report. 

# Features
You must be interested to know what are the key features of the Snowflake going to utilize under this project. So these are external stage. It is going to be used to connect the ADLS account to the Snowflake to pull the data from ADLS. We also going to use the COPY command feature of Snowflake that helps to load the data. We are going to use task as well, which is used to automate data loading and processing workflows. Streams, which is going to be used to capture and process incremental changes. Time travel for data recovery and historical analysis. Zero-copy cloning for efficient environment provisioning for dev, test and prod. 

# Benefits
Scalability: because Snowflake is built to handle a huge amount of data. Flexibility: Snowflake also has the capability to serve to variety of data sources, JSON, Parquet, CSV, and many more. Another benefit of this architecture is the performance. Snowflake has a separation of storage and compute, which can help you to improve the query performance independently. It is also a cost efficient because we have to pay only for the storage and computation resources we are going to use. And lastly, the data governance. It has the centralized platform for implementing the data quality and security measures. Now, as we have developed the entire outline for how we are going to implement this project, in the next video, we are going to talk about the architecture of this end-to-end project. See you there.

# Snowflake project architecture diagram
Selecting transcript lines in this section will navigate to timestamp in the video
Let's look at the high level architecture diagram of this project. You can see that the external system, like CRM, inventory, and transaction, the data is coming from all these three system to our storage account. That is Azure Data Lake Storage. And once the data comes in at Azure Data Lake Storage account, our work starts from there. It may be based on their frequency, like on a daily basis or an hourly basis. These systems will set the data into storage account. And the best part of the storage account is it can store any kind of a data. Now the data from the storage account will move to the Snowflake bronze layer schema. Now in the snowflake are one database we're going to create, at this one database will have three separate schema. I have showcased it using the database itself, but there are no three different database, there is just one database only, and one database will have three separate schema for bronze, silver, and gold. And the data will move in between them from bronze to silver, silver to gold. And once our gold layer data get created and available, this could be fed into the reporting tools. That's how the architecture of this project looks like. The major key components of this project, which we're going to build probably, is Snowflake database. We're going to create the external stage to move the data from ADLS to bronze. We need to create the multiple schema, bronze, silver, gold. We need to create the tables in all the layers. We need to create the task, which will automatically pull the data from ADLS to bronze and from bronze to silver. We also have a stream get created so that we can get the incremental data from bronze to silver. And lastly, we'll have views in our gold layer schema. Great, so we got a plan. What are we are going to do? Let's start implementing.

