# PacificRetail Snowflake Data Engineering Project
[![Azure](https://img.shields.io/badge/Cloud-Azure-0078D4?logo=microsoft-azure)](https://azure.microsoft.com/)
[![Snowflake](https://img.shields.io/badge/Warehouse-Snowflake-29B5E8?logo=snowflake)](https://www.snowflake.com/)
[![SQL](https://img.shields.io/badge/Query-SQL-336791?logo=postgresql)]()
[![Power BI](https://img.shields.io/badge/Visualization-Power%20BI-F2C811?logo=power-bi)](https://powerbi.microsoft.com/)
## ğŸ‘¤ My Role
As a **Data Engineer**, I designed and implemented this end-to-end Snowflake solution for PacificRetail.  
My responsibilities included:  
- Architecting the **Bronzeâ€“Silverâ€“Gold layered data pipeline**  
- Implementing **data ingestion** from Azure Data Lake Storage (ADLS) into Snowflake  
- Developing **incremental processing** with Snowflake **Tasks & Streams**  
- Building **data quality checks & standardization logic**  
- Designing **business-ready data models** in the Gold layer for analytics and ML use cases  

---

## ğŸ“Œ Project Overview
PacificRetail is a fast-growing e-commerce company operating in **15 countries** with:  
- **5M+ active customers**  
- **100K+ product catalog**  
- **Real-time transaction logs**  

The company faced challenges with **data silos, processing delays, poor scalability, and limited analytics capabilities**.  
This project showcases how Snowflakeâ€™s cloud-native architecture can overcome these bottlenecks and **enable near real-time reporting, advanced analytics, and ML readiness**.  

---

## ğŸ¯ Business Goals
- **Reduce reporting latency** from **24h â†’ 1h**  
- Achieve **99% accuracy** in cross-channel sales reporting  
- Support **5x growth** in data volumes with no performance degradation  
- Provide **self-service BI** to business teams  
- Build a foundation for **AI/ML use cases** (e.g., churn prediction, demand forecasting)  

---

## ğŸ—ï¸ Solution Architecture
![Architecture Diagram](assets/images/Azure_Diagram.png)
The solution uses a **multi-layer Snowflake architecture** with **ADLS as staging**:

1. **Data Sources**  
   - CRM system â†’ CSV (daily export)  
   - Inventory management â†’ JSON (hourly updates)  
   - E-commerce platform â†’ Parquet (real-time logs)  

2. **Azure Data Lake Storage (ADLS)**  
   - Central landing zone for all raw data  
   - Cost-effective, scalable, supports multiple file formats  

3. **Snowflake Data Warehouse**  
   - **Bronze Schema** â†’ Raw ingestion (append-only)  
   - **Silver Schema** â†’ Clean & standardized data with incremental merges  
   - **Gold Schema** â†’ Business-ready aggregates and models for BI/ML  

4. **Analytics Layer**  
   - Power BI / Tableau / Excel connected to Snowflake Gold layer  

---

## ğŸ”„ Data Flow
1. Data lands in **ADLS** from source systems  
2. Snowflake **COPY INTO** loads data â†’ **Bronze tables**  
3. Snowflake **Tasks + Streams** apply transformations â†’ **Silver tables**  
4. Standardization & data quality checks applied  
5. Aggregates, views, and marts created â†’ **Gold schema**  
6. BI/Analytics tools query the **Gold layer**  
![Architecture Diagram](assets/images/data_flow.png)
---

## ğŸ› ï¸ Key Snowflake Features Implemented
- **External Stages** â†’ Connect Snowflake with ADLS  
- **COPY Command** â†’ Load CSV, JSON, Parquet into Bronze tables  
- **Streams + Tasks** â†’ Automate incremental processing & transformations  
- **MERGE Statements** â†’ Handle updates in Silver schema  
- **Time Travel** â†’ Historical data recovery and audit  
- **Zero-Copy Cloning** â†’ Rapidly provision Dev/Test environments  

---

## ğŸ’» Example SQL Snippets

**1. Create External Stage (ADLS â†’ Snowflake)**  
![Architecture Diagram](assets/images/azure_adls.png)
```sql
create or replace stage adls_stage
url='azure://<storage_account>/<container>'
credentials=(azure_sas_token='<token>');
```
**2. Load Data into Bronze Layer**
![Architecture Diagram](assets/images/snowflake_bronze.png)
```sql
copy into bronze.customer
from @adls_stage/customer/
file_format = (type = csv field_optionally_enclosed_by='"')
on_error = 'continue';
```
**3. Incremental Merge into Silver Layer**
![Architecture Diagram](assets/images/snowflake_silver.png)
```sql
merge into silver.customer as target
using (
  select customer_id, name, email, country, customer_type
  from bronze.customer
) as source
on target.customer_id = source.customer_id
when matched then update set
    name = source.name,
    email = source.email,
    country = source.country,
    customer_type = source.customer_type
when not matched then insert (customer_id, name, email, country, customer_type)
values (source.customer_id, source.name, source.email, source.country, source.customer_type);
```
**4. Create Business Aggregates in Gold Layer**  
![Gold Layer Diagram](assets/images/snowflake_gold.png)
```sql
create or replace table gold.sales_summary as
select 
    c.country,
    p.category,
    date_trunc('day', o.transaction_date) as sales_date,
    count(distinct o.transaction_id) as total_orders,
    sum(o.total_amount) as total_revenue,
    avg(o.total_amount) as avg_order_value
from silver.orders o
join silver.customer c
    on o.customer_id = c.customer_id
join silver.product p
    on o.product_id = p.product_id
group by c.country, p.category, date_trunc('day', o.transaction_date);
```
## âœ… Business Outcomes
- â±ï¸ Reporting latency reduced from 24h â†’ 1h
- ğŸ“Š 99% accuracy in cross-channel reporting
- âš¡ 5x scalability with no performance degradation
- ğŸ‘©â€ğŸ’» Self-service analytics enabled for business teams
- ğŸ¤– ML-ready foundation for churn prediction, demand forecasting & personalization

## ğŸš€ Key Learnings
- Importance of layered architecture (Bronze/Silver/Gold) for flexibility
- Using Streams + Tasks to automate near real-time processing
- How Snowflakeâ€™s separation of storage & compute improves scalability
- Designing pipelines that balance cost efficiency with performance